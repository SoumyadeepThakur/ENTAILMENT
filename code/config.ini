[global]
logs = dumps/
path = dumps/model
model = attention
vocabsize = 10000
classes = 3
steps = 20
batch = 50
iterations = 10
frequency = 1000

[baseline]
vocabsize = ${global:vocabsize}
wvecsize = 100
depth = 4
steps = ${global:steps}
batch = ${global:batch}
deepness = 2
classes = ${global:classes}
mslrate = 1e-2
msdrate = 0.99
msdstep = 1000
msoptim = AdamOptimizer
celrate = 1e-2
cedrate = 0.99
cedstep = 1000
ceoptim = AdamOptimizer

[bidirectional]
vocabsize = ${global:vocabsize}
wvecsize = 100
depth = 4
steps = ${global:steps}
batch = ${global:batch}
deepness = 2
classes = ${global:classes}
mslrate = 1e-2
msdrate = 0.99
msdstep = 1000
msoptim = AdamOptimizer
celrate = 1e-2
cedrate = 0.99
cedstep = 1000
ceoptim = AdamOptimizer

[attention]
vocabsize = ${global:vocabsize}
wvecsize = 100
depth = 4
steps = ${global:steps}
memory = ${global:steps}
batch = ${global:batch}
deepness = 2
classes = ${global:classes}
mslrate = 1e-2
msdrate = 0.99
msdstep = 1000
msoptim = AdamOptimizer
celrate = 1e-2
cedrate = 0.99
cedstep = 1000
ceoptim = AdamOptimizer

[basic-tree]
vocabsize = ${global:vocabsize}
wvecsize = 100
depth = 4
steps = ${global:steps}
memory = ${global:steps}
batch = ${global:batch}
deepness = 2
classes = ${global:classes}
mslrate = 1e-2
msdrate = 0.99
msdstep = 1000
msoptim = AdamOptimizer
celrate = 1e-2
cedrate = 0.99
cedstep = 1000
ceoptim = AdamOptimizers
